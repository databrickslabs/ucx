============================= test session starts ==============================
created: 10/10 workers
10 workers [114 items]

ss.ss...........s.....................s.......FFFFFF.sF...FFFFFFF.FFF... [ 63%]
FFF.F...FF...F.F......sF................s.                               [100%]
=================================== FAILURES ===================================
____________________________ test_pipeline_crawler _____________________________
[gw6] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135158.8512619, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1035de890>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_pipeline = <function factory.<locals>.inner at 0x1035d9bd0>
inventory_schema = 'ucx_si6b7'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x103724820>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_pipeline_crawler(ws, make_pipeline, inventory_schema, sql_backend):
        logger.info("setting up fixtures")
>       created_pipeline = make_pipeline(configuration=_PIPELINE_CONF)

assessment/test_pipelines.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:796: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135158.8512619, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:40:56[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_si6b7[0m
[90m16:40:58[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_si6b7: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_si6b7[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_si6b7', metastore_id=None, name='ucx_si6b7', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:40 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_si6b7
16:40 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_si6b7: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_si6b7
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_si6b7', metastore_id=None, name='ucx_si6b7', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:40:58[0m [1m[32m INFO[0m [1m[t.i.assessment.test_assessment] setting up fixtures[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-qtyR.py[0m
------------------------------ Captured log call -------------------------------
16:40 INFO [tests.integration.assessment.test_assessment] setting up fixtures
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-qtyR.py
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:04[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:46:04[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_si6b7', metastore_id=None, name='ucx_si6b7', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:46:04[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_si6b7 CASCADE[0m
[90m16:46:05[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:46:05[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 notebook fixtures[0m
[90m16:46:05[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-qtyR.py[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_si6b7', metastore_id=None, name='ucx_si6b7', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_si6b7 CASCADE
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 notebook fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-qtyR.py
_______________________________ test_spn_crawler _______________________________
[gw2] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135158.677579, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1101dea10>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
inventory_schema = 'ucx_sf48j'
make_job = <function factory.<locals>.inner at 0x1101d9d80>
make_pipeline = <function factory.<locals>.inner at 0x1101d9f30>
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1102283d0>

    @retried(on=[NotFound], timeout=timedelta(minutes=3))
    def test_spn_crawler(ws, inventory_schema, make_job, make_pipeline, sql_backend):
>       make_job(spark_conf=_SPARK_CONF)

assessment/test_CLOUD_ENV.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:740: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135158.677579, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:40:56[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sf48j[0m
[90m16:40:58[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sf48j: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sf48j[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sf48j', metastore_id=None, name='ucx_sf48j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:40 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sf48j
16:40 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sf48j: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sf48j
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sf48j', metastore_id=None, name='ucx_sf48j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sf48j', metastore_id=None, name='ucx_sf48j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sf48j CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sf48j', metastore_id=None, name='ucx_sf48j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sf48j CASCADE
___________________ test_spn_crawler_with_available_secrets ____________________
[gw4] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135168.7766201, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x103ce7070>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
inventory_schema = 'ucx_sbzxk'
make_job = <function factory.<locals>.inner at 0x103ce9cf0>
make_pipeline = <function factory.<locals>.inner at 0x103ce9ea0>
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x103d349d0>
make_secret_scope = <function factory.<locals>.inner at 0x103cea050>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_spn_crawler_with_available_secrets(
        ws, inventory_schema, make_job, make_pipeline, sql_backend, make_secret_scope
    ):
        secret_scope = make_secret_scope()
        client_id_secret_key = "spn_client_id"
        client_secret_secret_key = "spn_client_secret"
        tenant_id_secret_key = "spn_tenant_id"
        ws.secrets.put_secret(secret_scope, client_id_secret_key, string_value="New_Application_Id")
        ws.secrets.put_secret(secret_scope, client_secret_secret_key, string_value="secret")
        ws.secrets.put_secret(
            secret_scope,
            tenant_id_secret_key,
            string_value=f"https://login.microsoftonline.com/{_TEST_TENANT_ID}/oauth2/token",
        )
        conf_secret_available = {
            "fs.CLOUD_ENV.account.oauth2.client.id.SA1.dfs.core.windows.net": f"{{{{secrets/{secret_scope}/{client_id_secret_key}}}}}",
            "fs.CLOUD_ENV.account.oauth2.client.secret.SA1.dfs.core.windows.net": f"{{{{secrets/{secret_scope}/{client_secret_secret_key}}}}}",
            "fs.CLOUD_ENV.account.oauth2.client.endpoint.SA1.dfs.core.windows.net": f"{{{{secrets/{secret_scope}/{tenant_id_secret_key}}}}}",
        }
>       make_job()

assessment/test_CLOUD_ENV.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:755: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135168.7766201, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:40:56[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sbzxk[0m
[90m16:41:07[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sbzxk: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sbzxk[0m
[90m16:41:07[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sbzxk', metastore_id=None, name='ucx_sbzxk', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:40 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sbzxk
16:41 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sbzxk: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sbzxk
16:41 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sbzxk', metastore_id=None, name='ucx_sbzxk', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:41:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added secret scope fixture: sdk-2LFS[0m
------------------------------ Captured log call -------------------------------
16:41 DEBUG [databricks.labs.ucx.mixins.fixtures] added secret scope fixture: sdk-2LFS
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 secret scope fixtures[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing secret scope fixture: sdk-2LFS[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sbzxk', metastore_id=None, name='ucx_sbzxk', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:46:18[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sbzxk CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 secret scope fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing secret scope fixture: sdk-2LFS
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sbzxk', metastore_id=None, name='ucx_sbzxk', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sbzxk CASCADE
___________________________________ test_job ___________________________________
[gw9] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135173.152318, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x11107d1b0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

make_job = <function factory.<locals>.inner at 0x110ede440>

    def test_job(make_job):
>       logger.info(f"created {make_job()}")

framework/test_fixtures.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:755: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135173.152318, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:22[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:46:22[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
______________________ test_cluster_crawler_no_isolation _______________________
[gw5] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135189.794497, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x107056b30>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_cluster = <function factory.<locals>.inner at 0x107059900>
inventory_schema = 'ucx_sy6ac'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1070a4820>

    def test_cluster_crawler_no_isolation(ws, make_cluster, inventory_schema, sql_backend):
>       created_cluster = make_cluster(data_security_mode=DataSecurityMode.NONE, num_workers=1)

assessment/test_clusters.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:676: in create
    kwargs["node_type_id"] = ws.clusters.select_node_type(local_disk=True)
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135189.794497, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:40:56[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sy6ac[0m
[90m16:40:58[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sy6ac: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sy6ac[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sy6ac', metastore_id=None, name='ucx_sy6ac', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:40 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sy6ac
16:40 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sy6ac: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sy6ac
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sy6ac', metastore_id=None, name='ucx_sy6ac', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:36[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:46:36[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sy6ac', metastore_id=None, name='ucx_sy6ac', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:46:36[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sy6ac CASCADE[0m
[90m16:46:37[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster fixtures[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sy6ac', metastore_id=None, name='ucx_sy6ac', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sy6ac CASCADE
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster fixtures
___________________ test_spn_crawler_deleted_cluster_policy ____________________
[gw3] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135190.6064498, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1054f6c80>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
inventory_schema = 'ucx_szqjw'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1055448e0>
make_job = <function factory.<locals>.inner at 0x1054f9d80>
make_cluster = <function factory.<locals>.inner at 0x1054f9f30>
make_cluster_policy = <function factory.<locals>.inner at 0x1054fa0e0>
make_random = <function make_random.<locals>.inner at 0x1054f9870>
make_notebook = <function factory.<locals>.inner at 0x1054f9bd0>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_spn_crawler_deleted_cluster_policy(
        ws,
        inventory_schema,
        sql_backend,
        make_job,
        make_cluster,
        make_cluster_policy,
        make_random,
        make_notebook,
    ):
        cluster_policy_id = make_cluster_policy().policy_id
>       make_cluster(single_node=True, spark_conf=_SPARK_CONF, policy_id=cluster_policy_id)

assessment/test_CLOUD_ENV.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:676: in create
    kwargs["node_type_id"] = ws.clusters.select_node_type(local_disk=True)
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135190.6064498, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:40:56[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_szqjw[0m
[90m16:40:58[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_szqjw: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_szqjw[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_szqjw', metastore_id=None, name='ucx_szqjw', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:40 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_szqjw
16:40 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_szqjw: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_szqjw
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_szqjw', metastore_id=None, name='ucx_szqjw', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:40:58[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/000AF8E53C989EDE[0m
[90m16:40:58[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='000AF8E53C989EDE')[0m
------------------------------ Captured log call -------------------------------
16:40 INFO [databricks.labs.ucx.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/000AF8E53C989EDE
16:40 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='000AF8E53C989EDE')
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster policy fixtures[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='000AF8E53C989EDE')[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster fixtures[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_szqjw', metastore_id=None, name='ucx_szqjw', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:46:39[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_szqjw CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster policy fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='000AF8E53C989EDE')
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_szqjw', metastore_id=None, name='ucx_szqjw', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_szqjw CASCADE
______________________________ test_instance_pool ______________________________
[gw1] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135201.591984, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x10970d7e0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

make_instance_pool = <function factory.<locals>.inner at 0x109831ea0>

    def test_instance_pool(make_instance_pool):
>       logger.info(f"created {make_instance_pool()}")

framework/test_fixtures.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:717: in create
    node_type_id = ws.clusters.select_node_type(local_disk=True)
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135201.591984, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
--------------------------- Captured stderr teardown ---------------------------
[90m16:46:50[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 instance pool fixtures[0m
---------------------------- Captured log teardown -----------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 instance pool fixtures
__________________________ test_spn_crawler_no_config __________________________
[gw2] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135467.878012, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x110279d50>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
inventory_schema = 'ucx_s5exd'
make_job = <function factory.<locals>.inner at 0x1104bdea0>
make_pipeline = <function factory.<locals>.inner at 0x1104be050>
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x11027af50>
make_cluster = <function factory.<locals>.inner at 0x1104be200>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_spn_crawler_no_config(ws, inventory_schema, make_job, make_pipeline, sql_backend, make_cluster):
>       make_job()

assessment/test_CLOUD_ENV.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:755: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135467.878012, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s5exd[0m
[90m16:46:07[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_s5exd: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s5exd[0m
[90m16:46:07[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s5exd', metastore_id=None, name='ucx_s5exd', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s5exd
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_s5exd: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s5exd
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s5exd', metastore_id=None, name='ucx_s5exd', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster fixtures[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s5exd', metastore_id=None, name='ucx_s5exd', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:08[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s5exd CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s5exd', metastore_id=None, name='ucx_s5exd', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s5exd CASCADE
____________________ test_pipeline_with_secret_conf_crawler ____________________
[gw6] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135467.2994509, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x104a50b50>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_pipeline = <function factory.<locals>.inner at 0x103a3bbe0>
inventory_schema = 'ucx_s7yki'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x104a51930>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_pipeline_with_secret_conf_crawler(ws, make_pipeline, inventory_schema, sql_backend):
        logger.info("setting up fixtures")
>       created_pipeline = make_pipeline(configuration=_PIPELINE_CONF_WITH_SECRET)

assessment/test_pipelines.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:796: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135467.2994509, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:46:05[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s7yki[0m
[90m16:46:06[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_s7yki: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s7yki[0m
[90m16:46:06[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s7yki', metastore_id=None, name='ucx_s7yki', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s7yki
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_s7yki: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s7yki
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s7yki', metastore_id=None, name='ucx_s7yki', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:46:06[0m [1m[32m INFO[0m [1m[t.i.assessment.test_assessment] setting up fixtures[0m
[90m16:46:07[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-NvG6.py[0m
------------------------------ Captured log call -------------------------------
16:46 INFO [tests.integration.assessment.test_assessment] setting up fixtures
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-NvG6.py
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:15[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:51:15[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s7yki', metastore_id=None, name='ucx_s7yki', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:15[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s7yki CASCADE[0m
[90m16:51:16[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:51:16[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 notebook fixtures[0m
[90m16:51:16[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-NvG6.py[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s7yki', metastore_id=None, name='ucx_s7yki', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s7yki CASCADE
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 notebook fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-NvG6.py
________________________________ test_pipeline _________________________________
[gw9] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135483.005255, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x11107e1a0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

make_pipeline = <function factory.<locals>.inner at 0x11147b7f0>

    def test_pipeline(make_pipeline):
>       logger.info(f"created {make_pipeline()}")

framework/test_fixtures.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:796: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'None/None databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135483.005255, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
----------------------------- Captured stderr call -----------------------------
[90m16:46:23[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-Zmmr.py[0m
------------------------------ Captured log call -------------------------------
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-Zmmr.py
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:51:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 notebook fixtures[0m
[90m16:51:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-Zmmr.py[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 notebook fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-Zmmr.py
_____________________________ test_cluster_crawler _____________________________
[gw4] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135480.574402, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x105158880>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_cluster = <function factory.<locals>.inner at 0x103ce9ab0>
inventory_schema = 'ucx_sjq85'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x10515a7a0>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_cluster_crawler(ws, make_cluster, inventory_schema, sql_backend):
>       created_cluster = make_cluster(single_node=True, spark_conf=_SPARK_CONF)

assessment/test_clusters.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:676: in create
    kwargs["node_type_id"] = ws.clusters.select_node_type(local_disk=True)
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135480.574402, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:46:19[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sjq85[0m
[90m16:46:20[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sjq85: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sjq85[0m
[90m16:46:20[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sjq85', metastore_id=None, name='ucx_sjq85', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sjq85
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sjq85: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sjq85
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sjq85', metastore_id=None, name='ucx_sjq85', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:51:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sjq85', metastore_id=None, name='ucx_sjq85', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:28[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sjq85 CASCADE[0m
[90m16:51:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster fixtures[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sjq85', metastore_id=None, name='ucx_sjq85', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sjq85 CASCADE
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster fixtures
______________ test_spn_crawler_with_pipeline_unavailable_secret _______________
[gw3] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135501.3791869, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1055d8a00>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
inventory_schema = 'ucx_s746j'
make_job = <function factory.<locals>.inner at 0x10581a200>
make_pipeline = <function factory.<locals>.inner at 0x10581a3b0>
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1055daad0>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_spn_crawler_with_pipeline_unavailable_secret(ws, inventory_schema, make_job, make_pipeline, sql_backend):
>       make_job(spark_conf=_SPARK_CONF)

assessment/test_CLOUD_ENV.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:740: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135501.3791869, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:46:40[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s746j[0m
[90m16:46:41[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_s746j: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s746j[0m
[90m16:46:41[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s746j', metastore_id=None, name='ucx_s746j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s746j
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_s746j: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s746j
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s746j', metastore_id=None, name='ucx_s746j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s746j', metastore_id=None, name='ucx_s746j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s746j CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s746j', metastore_id=None, name='ucx_s746j', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s746j CASCADE
_______________________________ test_job_crawler _______________________________
[gw5] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135498.259263, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1070f06a0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_job = <function factory.<locals>.inner at 0x10754fac0>
inventory_schema = 'ucx_sb2rp'
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1070f34c0>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_job_crawler(ws, make_job, inventory_schema, sql_backend):
>       new_job = make_job(spark_conf=_SPARK_CONF)

assessment/test_jobs.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:740: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135498.259263, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:46:37[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sb2rp[0m
[90m16:46:38[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sb2rp: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sb2rp[0m
[90m16:46:38[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb2rp', metastore_id=None, name='ucx_sb2rp', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:46 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sb2rp
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sb2rp: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sb2rp
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb2rp', metastore_id=None, name='ucx_sb2rp', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:47[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:51:47[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb2rp', metastore_id=None, name='ucx_sb2rp', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:47[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sb2rp CASCADE[0m
[90m16:51:48[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:51:48[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb2rp', metastore_id=None, name='ucx_sb2rp', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sb2rp CASCADE
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
_____________________________ test_instance_pools ______________________________
[gw7] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135511.168194, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x103bdb9d0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_group = <function factory.<locals>.inner at 0x103bde710>
make_instance_pool = <function factory.<locals>.inner at 0x103bde5f0>
make_instance_pool_permissions = <function factory.<locals>.inner at 0x103bdecb0>

    @retried(on=[NotFound], timeout=timedelta(minutes=3))
    def test_instance_pools(ws, make_group, make_instance_pool, make_instance_pool_permissions):
        group_a = make_group()
        group_b = make_group()
>       pool = make_instance_pool()

workspace_access/test_generic.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:717: in create
    node_type_id = ws.clusters.select_node_type(local_disk=True)
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135511.168194, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
----------------------------- Captured stderr call -----------------------------
[90m16:46:50[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-G2mC: https://DATABRICKS_HOST#setting/accounts/groups/488915071483829[0m
[90m16:46:50[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-G2mC', entitlements=[], external_id=None, groups=[], id='488915071483829', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:46:51[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-yx8k: https://DATABRICKS_HOST#setting/accounts/groups/247040551974459[0m
[90m16:46:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-yx8k', entitlements=[], external_id=None, groups=[], id='247040551974459', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
------------------------------ Captured log call -------------------------------
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-G2mC: https://DATABRICKS_HOST#setting/accounts/groups/488915071483829
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-G2mC', entitlements=[], external_id=None, groups=[], id='488915071483829', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-yx8k: https://DATABRICKS_HOST#setting/accounts/groups/247040551974459
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-yx8k', entitlements=[], external_id=None, groups=[], id='247040551974459', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
--------------------------- Captured stderr teardown ---------------------------
[90m16:51:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 instance_pool permissions fixtures[0m
[90m16:51:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 instance pool fixtures[0m
[90m16:51:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 2 workspace group fixtures[0m
[90m16:51:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-G2mC', entitlements=[], external_id=None, groups=[], id='488915071483829', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:51[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-yx8k', entitlements=[], external_id=None, groups=[], id='247040551974459', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
---------------------------- Captured log teardown -----------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 instance_pool permissions fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 instance pool fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 2 workspace group fixtures
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-G2mC', entitlements=[], external_id=None, groups=[], id='488915071483829', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-yx8k', entitlements=[], external_id=None, groups=[], id='247040551974459', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
__________ test_job_failure_propagates_correct_error_message_and_logs __________
[gw8] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135527.5640059, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x107d90700>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x107db9210>
new_installation = <function new_installation.<locals>.factory at 0x107ad9fc0>

    @retried(on=[NotFound, Unknown, TimeoutError], timeout=timedelta(minutes=5))
    def test_job_failure_propagates_correct_error_message_and_logs(ws, sql_backend, new_installation):
>       install = new_installation()

test_installation.py:103: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135527.5640059, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError

The above exception was the direct cause of the following exception:

args = ()
kwargs = {'new_installation': <function new_installation.<locals>.factory at 0x107ad9fc0>, 'sql_backend': <databricks.labs.ucx...., 'ws': WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)}
deadline = 1709135492.905547, attempt = 2
last_err = TimeoutError('Timed out after 0:05:00')
retry_reason = 'TimeoutError is allowed to retry', sleep = 1
retry_after_secs = None, err_type = <class 'TimeoutError'>

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:45:44[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sutru[0m
[90m16:46:32[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sutru: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sutru[0m
[90m16:46:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sutru', metastore_id=None, name='ucx_sutru', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:45 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sutru
16:46 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sutru: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sutru
16:46 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sutru', metastore_id=None, name='ucx_sutru', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:47:06[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.wbM7/config.yml) doesn't exist.[0m
[90m16:47:06[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:47:06[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:47:07[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:47 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.wbM7/config.yml) doesn't exist.
16:47 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:47 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:47 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:52:09[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:52:09[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:52:09[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sutru', metastore_id=None, name='ucx_sutru', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:52:09[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sutru CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sutru', metastore_id=None, name='ucx_sutru', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:52 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sutru CASCADE
__________________________________ test_jobs ___________________________________
[gw1] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135553.4105651, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x109a2b070>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_group = <function factory.<locals>.inner at 0x10a077880>
make_job = <function factory.<locals>.inner at 0x10a077b50>
make_job_permissions = <function factory.<locals>.inner at 0x10a077d00>

    @retried(on=[BadRequest], timeout=timedelta(minutes=3))
    def test_jobs(ws, make_group, make_job, make_job_permissions):
        group_a = make_group()
        group_b = make_group()
>       job = make_job()

workspace_access/test_generic.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:755: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135553.4105651, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
----------------------------- Captured stderr call -----------------------------
[90m16:47:33[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-2IM4: https://DATABRICKS_HOST#setting/accounts/groups/445847697381313[0m
[90m16:47:33[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-2IM4', entitlements=[], external_id=None, groups=[], id='445847697381313', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:47:33[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-l8ty: https://DATABRICKS_HOST#setting/accounts/groups/450864778332831[0m
[90m16:47:33[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-l8ty', entitlements=[], external_id=None, groups=[], id='450864778332831', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
------------------------------ Captured log call -------------------------------
16:47 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-2IM4: https://DATABRICKS_HOST#setting/accounts/groups/445847697381313
16:47 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-2IM4', entitlements=[], external_id=None, groups=[], id='445847697381313', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:47 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-l8ty: https://DATABRICKS_HOST#setting/accounts/groups/450864778332831
16:47 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-l8ty', entitlements=[], external_id=None, groups=[], id='450864778332831', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
--------------------------- Captured stderr teardown ---------------------------
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job permissions fixtures[0m
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 2 workspace group fixtures[0m
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-2IM4', entitlements=[], external_id=None, groups=[], id='445847697381313', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:52:42[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-l8ty', entitlements=[], external_id=None, groups=[], id='450864778332831', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
---------------------------- Captured log teardown -----------------------------
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job permissions fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 2 workspace group fixtures
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-2IM4', entitlements=[], external_id=None, groups=[], id='445847697381313', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-l8ty', entitlements=[], external_id=None, groups=[], id='450864778332831', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
_______________________ test_running_real_assessment_job _______________________
[gw0] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135602.162967, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1205c9e70>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
new_installation = <function new_installation.<locals>.factory at 0x107c4cf70>
make_ucx_group = <function make_ucx_group.<locals>.inner at 0x107c4e170>
make_cluster_policy = <function factory.<locals>.inner at 0x107c4dab0>
make_cluster_policy_permissions = <function factory.<locals>.inner at 0x107c4e7a0>

    @retried(on=[NotFound, Unknown, InvalidParameterValue], timeout=timedelta(minutes=10))
    def test_running_real_assessment_job(
        ws, new_installation, make_ucx_group, make_cluster_policy, make_cluster_policy_permissions
    ):
        ws_group_a, _ = make_ucx_group()
    
        cluster_policy = make_cluster_policy()
        make_cluster_policy_permissions(
            object_id=cluster_policy.policy_id,
            permission_level=PermissionLevel.CAN_USE,
            group_name=ws_group_a.display_name,
        )
    
>       install = new_installation(lambda wc: replace(wc, include_group_names=[ws_group_a.display_name]))

test_installation.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135602.162967, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:48:03[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s9qll[0m
[90m16:48:16[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_s9qll: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s9qll[0m
[90m16:48:16[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s9qll', metastore_id=None, name='ucx_s9qll', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:48 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s9qll
16:48 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_s9qll: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s9qll
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s9qll', metastore_id=None, name='ucx_s9qll', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:48:17[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-rph8@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-rph8@example.com')], entitlements=[], external_id=None, groups=[], id='3336535109379214', name=Name(family_name=None, given_name='sdk-rph8@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-rph8@example.com')[0m
[90m16:48:18[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group ucx_YWhf: https://DATABRICKS_HOST#setting/accounts/groups/882338301305760[0m
[90m16:48:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_YWhf', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='882338301305760', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:48:18[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Account group ucx_YWhf: https://accounts.CLOUD_ENVdatabricks.net/users/groups/884417745973879/members[0m
[90m16:48:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added account group fixture: Group(display_name='ucx_YWhf', entitlements=[], external_id=None, groups=[], id='884417745973879', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:48:19[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/001972056FC11395[0m
[90m16:48:19[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='001972056FC11395')[0m
[90m16:48:20[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster_policy permissions fixture: 001972056FC11395 [group_name admins CAN_USE] -> [group_name ucx_YWhf CAN_USE][0m
[90m16:48:21[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.vdVl/config.yml) doesn't exist.[0m
[90m16:48:21[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:48:21[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:48:22[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-rph8@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-rph8@example.com')], entitlements=[], external_id=None, groups=[], id='3336535109379214', name=Name(family_name=None, given_name='sdk-rph8@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-rph8@example.com')
16:48 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group ucx_YWhf: https://DATABRICKS_HOST#setting/accounts/groups/882338301305760
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_YWhf', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='882338301305760', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:48 INFO [databricks.labs.ucx.mixins.fixtures] Account group ucx_YWhf: https://accounts.CLOUD_ENVdatabricks.net/users/groups/884417745973879/members
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added account group fixture: Group(display_name='ucx_YWhf', entitlements=[], external_id=None, groups=[], id='884417745973879', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:48 INFO [databricks.labs.ucx.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/001972056FC11395
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='001972056FC11395')
16:48 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster_policy permissions fixture: 001972056FC11395 [group_name admins CAN_USE] -> [group_name ucx_YWhf CAN_USE]
16:48 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.vdVl/config.yml) doesn't exist.
16:48 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:48 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:48 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:53:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster_policy permissions fixtures[0m
[90m16:53:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster_policy permissions fixture: 001972056FC11395 [group_name admins CAN_USE] -> [group_name ucx_YWhf CAN_USE][0m
[90m16:53:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace user fixtures[0m
[90m16:53:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-rph8@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-rph8@example.com')], entitlements=[], external_id=None, groups=[], id='3336535109379214', name=Name(family_name=None, given_name='sdk-rph8@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-rph8@example.com')[0m
[90m16:53:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 account group fixtures[0m
[90m16:53:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing account group fixture: Group(display_name='ucx_YWhf', entitlements=[], external_id=None, groups=[], id='884417745973879', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:53:31[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:53:31[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_YWhf', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='882338301305760', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:53:31[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster policy fixtures[0m
[90m16:53:31[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='001972056FC11395')[0m
[90m16:53:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:53:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s9qll', metastore_id=None, name='ucx_s9qll', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:53:32[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s9qll CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster_policy permissions fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster_policy permissions fixture: 001972056FC11395 [group_name admins CAN_USE] -> [group_name ucx_YWhf CAN_USE]
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace user fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-rph8@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-rph8@example.com')], entitlements=[], external_id=None, groups=[], id='3336535109379214', name=Name(family_name=None, given_name='sdk-rph8@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-rph8@example.com')
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 account group fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing account group fixture: Group(display_name='ucx_YWhf', entitlements=[], external_id=None, groups=[], id='884417745973879', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_YWhf', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='882338301305760', members=[ComplexValue(display='sdk-rph8@example.com', primary=None, ref='Users/3336535109379214', type=None, value='3336535109379214')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster policy fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='001972056FC11395')
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:53 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s9qll', metastore_id=None, name='ucx_s9qll', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:53 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s9qll CASCADE
______________ test_running_real_validate_groups_permissions_job _______________
[gw2] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135777.569775, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1101dffa0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x1102d60e0>
new_installation = <function new_installation.<locals>.factory at 0x1104be3b0>
make_group = <function factory.<locals>.inner at 0x1104beb90>
make_query = <function factory.<locals>.inner at 0x1104be680>
make_query_permissions = <function factory.<locals>.inner at 0x1104bf130>

    @retried(on=[NotFound, Unknown, InvalidParameterValue], timeout=timedelta(minutes=5))
    def test_running_real_validate_groups_permissions_job(
        ws, sql_backend, new_installation, make_group, make_query, make_query_permissions
    ):
        ws_group_a = make_group()
    
        query = make_query()
        make_query_permissions(
            object_id=query.id,
            permission_level=sql.PermissionLevel.CAN_EDIT,
            group_name=ws_group_a.display_name,
        )
    
        redash_permissions = RedashPermissionsSupport(
            ws,
            [redash.Listing(ws.queries.list, sql.ObjectTypePlural.QUERIES)],
        )
    
>       install = new_installation(lambda wc: replace(wc, include_group_names=[ws_group_a.display_name]))

test_installation.py:229: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135777.569775, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:10[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_srich[0m
[90m16:51:11[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_srich: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_srich[0m
[90m16:51:11[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_srich', metastore_id=None, name='ucx_srich', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_srich
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_srich: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_srich
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_srich', metastore_id=None, name='ucx_srich', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:11[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-miU4: https://DATABRICKS_HOST#setting/accounts/groups/187326012744169[0m
[90m16:51:11[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-miU4', entitlements=[], external_id=None, groups=[], id='187326012744169', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:11[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sznu9[0m
[90m16:51:12[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sznu9: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sznu9[0m
[90m16:51:12[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sznu9', metastore_id=None, name='ucx_sznu9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:51:12[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE TABLE hive_metastore.ucx_sznu9.ucx_tykxc (id INT, value STRING)[0m
[90m16:51:13[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Table hive_metastore.ucx_sznu9.ucx_tykxc: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sznu9/ucx_tykxc[0m
[90m16:51:13[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added table fixture: TableInfo(access_point=None, catalog_name='hive_metastore', columns=None, comment=None, created_at=None, created_by=None, data_access_configuration_id=None, data_source_format=<DataSourceFormat.DELTA: 'DELTA'>, deleted_at=None, delta_runtime_properties_kvpairs=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, encryption_details=None, full_name='hive_metastore.ucx_sznu9.ucx_tykxc', metastore_id=None, name='ucx_tykxc', owner=None, pipeline_id=None, properties=None, row_filter=None, schema_name='ucx_sznu9', sql_path=None, storage_credential_name=None, storage_location='dbfs:/user/hive/warehouse/ucx_sznu9/ucx_tykxc', table_constraints=None, table_id=None, table_type=<TableType.MANAGED: 'MANAGED'>, updated_at=None, updated_by=None, view_definition=None, view_dependencies=None)[0m
[90m16:51:14[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Query Created ucx_query_Q8PiB: https://DATABRICKS_HOST/sql/editor/6422a9ed-e6a4-445c-b882-a4749a0d2e27[0m
[90m16:51:14[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added query fixture: Query(can_edit=None, created_at='2024-02-28T15:51:14Z', data_source_id=None, description='TEST QUERY FOR UCX', id='6422a9ed-e6a4-445c-b882-a4749a0d2e27', is_archived=False, is_draft=False, is_favorite=False, is_safe=True, last_modified_by=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), last_modified_by_id=None, latest_query_data_id=None, name='ucx_query_Q8PiB', options=QueryOptions(moved_to_trash_at=None, parameters=[]), parent='folders/2205353315927650', permission_tier=None, query='SELECT * FROM ucx_sznu9.ucx_tykxc', query_hash=None, run_as_role=<RunAsRole.OWNER: 'owner'>, tags=[], updated_at='2024-02-28T15:51:14Z', user=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), user_id=3752755683879704, visualizations=[Visualization(created_at='2024-02-28T15:51:14Z', description='', id='ef6401d1-e8ca-450c-a94d-7c9c3fbbd287', name='Results', options={'version': 2}, type='TABLE', updated_at='2024-02-28T15:51:14Z')])[0m
[90m16:51:15[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added query permissions fixture: 6422a9ed-e6a4-445c-b882-a4749a0d2e27 [user_name william.conti@databricks.com CAN_MANAGE, group_name admins CAN_MANAGE] -> [group_name sdk-miU4 CAN_EDIT][0m
[90m16:51:16[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.GlX2/config.yml) doesn't exist.[0m
[90m16:51:16[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:16[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:17[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-miU4: https://DATABRICKS_HOST#setting/accounts/groups/187326012744169
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-miU4', entitlements=[], external_id=None, groups=[], id='187326012744169', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sznu9
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sznu9: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sznu9
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sznu9', metastore_id=None, name='ucx_sznu9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE TABLE hive_metastore.ucx_sznu9.ucx_tykxc (id INT, value STRING)
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Table hive_metastore.ucx_sznu9.ucx_tykxc: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sznu9/ucx_tykxc
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added table fixture: TableInfo(access_point=None, catalog_name='hive_metastore', columns=None, comment=None, created_at=None, created_by=None, data_access_configuration_id=None, data_source_format=<DataSourceFormat.DELTA: 'DELTA'>, deleted_at=None, delta_runtime_properties_kvpairs=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, encryption_details=None, full_name='hive_metastore.ucx_sznu9.ucx_tykxc', metastore_id=None, name='ucx_tykxc', owner=None, pipeline_id=None, properties=None, row_filter=None, schema_name='ucx_sznu9', sql_path=None, storage_credential_name=None, storage_location='dbfs:/user/hive/warehouse/ucx_sznu9/ucx_tykxc', table_constraints=None, table_id=None, table_type=<TableType.MANAGED: 'MANAGED'>, updated_at=None, updated_by=None, view_definition=None, view_dependencies=None)
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Query Created ucx_query_Q8PiB: https://DATABRICKS_HOST/sql/editor/6422a9ed-e6a4-445c-b882-a4749a0d2e27
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added query fixture: Query(can_edit=None, created_at='2024-02-28T15:51:14Z', data_source_id=None, description='TEST QUERY FOR UCX', id='6422a9ed-e6a4-445c-b882-a4749a0d2e27', is_archived=False, is_draft=False, is_favorite=False, is_safe=True, last_modified_by=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), last_modified_by_id=None, latest_query_data_id=None, name='ucx_query_Q8PiB', options=QueryOptions(moved_to_trash_at=None, parameters=[]), parent='folders/2205353315927650', permission_tier=None, query='SELECT * FROM ucx_sznu9.ucx_tykxc', query_hash=None, run_as_role=<RunAsRole.OWNER: 'owner'>, tags=[], updated_at='2024-02-28T15:51:14Z', user=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), user_id=3752755683879704, visualizations=[Visualization(created_at='2024-02-28T15:51:14Z', description='', id='ef6401d1-e8ca-450c-a94d-7c9c3fbbd287', name='Results', options={'version': 2}, type='TABLE', updated_at='2024-02-28T15:51:14Z')])
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added query permissions fixture: 6422a9ed-e6a4-445c-b882-a4749a0d2e27 [user_name william.conti@databricks.com CAN_MANAGE, group_name admins CAN_MANAGE] -> [group_name sdk-miU4 CAN_EDIT]
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.GlX2/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 query permissions fixtures[0m
[90m16:56:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing query permissions fixture: 6422a9ed-e6a4-445c-b882-a4749a0d2e27 [user_name william.conti@databricks.com CAN_MANAGE, group_name admins CAN_MANAGE] -> [group_name sdk-miU4 CAN_EDIT][0m
[90m16:56:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 query fixtures[0m
[90m16:56:27[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing query fixture: Query(can_edit=None, created_at='2024-02-28T15:51:14Z', data_source_id=None, description='TEST QUERY FOR UCX', id='6422a9ed-e6a4-445c-b882-a4749a0d2e27', is_archived=False, is_draft=False, is_favorite=False, is_safe=True, last_modified_by=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), last_modified_by_id=None, latest_query_data_id=None, name='ucx_query_Q8PiB', options=QueryOptions(moved_to_trash_at=None, parameters=[]), parent='folders/2205353315927650', permission_tier=None, query='SELECT * FROM ucx_sznu9.ucx_tykxc', query_hash=None, run_as_role=<RunAsRole.OWNER: 'owner'>, tags=[], updated_at='2024-02-28T15:51:14Z', user=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), user_id=3752755683879704, visualizations=[Visualization(created_at='2024-02-28T15:51:14Z', description='', id='ef6401d1-e8ca-450c-a94d-7c9c3fbbd287', name='Results', options={'version': 2}, type='TABLE', updated_at='2024-02-28T15:51:14Z')])[0m
[90m16:56:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 table fixtures[0m
[90m16:56:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing table fixture: TableInfo(access_point=None, catalog_name='hive_metastore', columns=None, comment=None, created_at=None, created_by=None, data_access_configuration_id=None, data_source_format=<DataSourceFormat.DELTA: 'DELTA'>, deleted_at=None, delta_runtime_properties_kvpairs=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, encryption_details=None, full_name='hive_metastore.ucx_sznu9.ucx_tykxc', metastore_id=None, name='ucx_tykxc', owner=None, pipeline_id=None, properties=None, row_filter=None, schema_name='ucx_sznu9', sql_path=None, storage_credential_name=None, storage_location='dbfs:/user/hive/warehouse/ucx_sznu9/ucx_tykxc', table_constraints=None, table_id=None, table_type=<TableType.MANAGED: 'MANAGED'>, updated_at=None, updated_by=None, view_definition=None, view_dependencies=None)[0m
[90m16:56:28[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP TABLE IF EXISTS hive_metastore.ucx_sznu9.ucx_tykxc[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-miU4', entitlements=[], external_id=None, groups=[], id='187326012744169', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 2 schema fixtures[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_srich', metastore_id=None, name='ucx_srich', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:29[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_srich CASCADE[0m
[90m16:56:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sznu9', metastore_id=None, name='ucx_sznu9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:30[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sznu9 CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 query permissions fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing query permissions fixture: 6422a9ed-e6a4-445c-b882-a4749a0d2e27 [user_name william.conti@databricks.com CAN_MANAGE, group_name admins CAN_MANAGE] -> [group_name sdk-miU4 CAN_EDIT]
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 query fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing query fixture: Query(can_edit=None, created_at='2024-02-28T15:51:14Z', data_source_id=None, description='TEST QUERY FOR UCX', id='6422a9ed-e6a4-445c-b882-a4749a0d2e27', is_archived=False, is_draft=False, is_favorite=False, is_safe=True, last_modified_by=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), last_modified_by_id=None, latest_query_data_id=None, name='ucx_query_Q8PiB', options=QueryOptions(moved_to_trash_at=None, parameters=[]), parent='folders/2205353315927650', permission_tier=None, query='SELECT * FROM ucx_sznu9.ucx_tykxc', query_hash=None, run_as_role=<RunAsRole.OWNER: 'owner'>, tags=[], updated_at='2024-02-28T15:51:14Z', user=User(email='william.conti@databricks.com', id=3752755683879704, name='William Conti'), user_id=3752755683879704, visualizations=[Visualization(created_at='2024-02-28T15:51:14Z', description='', id='ef6401d1-e8ca-450c-a94d-7c9c3fbbd287', name='Results', options={'version': 2}, type='TABLE', updated_at='2024-02-28T15:51:14Z')])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 table fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing table fixture: TableInfo(access_point=None, catalog_name='hive_metastore', columns=None, comment=None, created_at=None, created_by=None, data_access_configuration_id=None, data_source_format=<DataSourceFormat.DELTA: 'DELTA'>, deleted_at=None, delta_runtime_properties_kvpairs=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, encryption_details=None, full_name='hive_metastore.ucx_sznu9.ucx_tykxc', metastore_id=None, name='ucx_tykxc', owner=None, pipeline_id=None, properties=None, row_filter=None, schema_name='ucx_sznu9', sql_path=None, storage_credential_name=None, storage_location='dbfs:/user/hive/warehouse/ucx_sznu9/ucx_tykxc', table_constraints=None, table_id=None, table_type=<TableType.MANAGED: 'MANAGED'>, updated_at=None, updated_by=None, view_definition=None, view_dependencies=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP TABLE IF EXISTS hive_metastore.ucx_sznu9.ucx_tykxc
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-miU4', entitlements=[], external_id=None, groups=[], id='187326012744169', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 2 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_srich', metastore_id=None, name='ucx_srich', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_srich CASCADE
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sznu9', metastore_id=None, name='ucx_sznu9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sznu9 CASCADE
__________________ test_running_real_remove_backup_groups_job __________________
[gw9] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135792.115157, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1115159c0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x111514070>
new_installation = <function new_installation.<locals>.factory at 0x111479870>
make_ucx_group = <function make_ucx_group.<locals>.inner at 0x11147a170>

    @retried(on=[NotFound, InvalidParameterValue], timeout=timedelta(minutes=5))
    def test_running_real_remove_backup_groups_job(ws, sql_backend, new_installation, make_ucx_group):
        ws_group_a, _ = make_ucx_group()
    
>       install = new_installation(lambda wc: replace(wc, include_group_names=[ws_group_a.display_name]))

test_installation.py:275: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135792.115157, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:27[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sb7b4[0m
[90m16:51:28[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sb7b4: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sb7b4[0m
[90m16:51:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb7b4', metastore_id=None, name='ucx_sb7b4', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sb7b4
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sb7b4: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sb7b4
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb7b4', metastore_id=None, name='ucx_sb7b4', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:28[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-bnul@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-bnul@example.com')], entitlements=[], external_id=None, groups=[], id='275811120571425', name=Name(family_name=None, given_name='sdk-bnul@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-bnul@example.com')[0m
[90m16:51:29[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group ucx_36Hv: https://DATABRICKS_HOST#setting/accounts/groups/412911073544250[0m
[90m16:51:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_36Hv', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='412911073544250', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:30[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Account group ucx_36Hv: https://accounts.CLOUD_ENVdatabricks.net/users/groups/610322502169631/members[0m
[90m16:51:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added account group fixture: Group(display_name='ucx_36Hv', entitlements=[], external_id=None, groups=[], id='610322502169631', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:31[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.qEIQ/config.yml) doesn't exist.[0m
[90m16:51:31[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:31[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:31[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-bnul@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-bnul@example.com')], entitlements=[], external_id=None, groups=[], id='275811120571425', name=Name(family_name=None, given_name='sdk-bnul@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-bnul@example.com')
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group ucx_36Hv: https://DATABRICKS_HOST#setting/accounts/groups/412911073544250
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_36Hv', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='412911073544250', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Account group ucx_36Hv: https://accounts.CLOUD_ENVdatabricks.net/users/groups/610322502169631/members
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added account group fixture: Group(display_name='ucx_36Hv', entitlements=[], external_id=None, groups=[], id='610322502169631', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.qEIQ/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace user fixtures[0m
[90m16:56:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-bnul@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-bnul@example.com')], entitlements=[], external_id=None, groups=[], id='275811120571425', name=Name(family_name=None, given_name='sdk-bnul@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-bnul@example.com')[0m
[90m16:56:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 account group fixtures[0m
[90m16:56:32[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing account group fixture: Group(display_name='ucx_36Hv', entitlements=[], external_id=None, groups=[], id='610322502169631', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_36Hv', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='412911073544250', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb7b4', metastore_id=None, name='ucx_sb7b4', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sb7b4 CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace user fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-bnul@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-bnul@example.com')], entitlements=[], external_id=None, groups=[], id='275811120571425', name=Name(family_name=None, given_name='sdk-bnul@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-bnul@example.com')
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 account group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing account group fixture: Group(display_name='ucx_36Hv', entitlements=[], external_id=None, groups=[], id='610322502169631', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_36Hv', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='412911073544250', members=[ComplexValue(display='sdk-bnul@example.com', primary=None, ref='Users/275811120571425', type=None, value='275811120571425')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sb7b4', metastore_id=None, name='ucx_sb7b4', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sb7b4 CASCADE
___________ test_running_real_validate_groups_permissions_job_fails ____________
[gw4] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135793.213341, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x103ce65c0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x103de6ef0>
new_installation = <function new_installation.<locals>.factory at 0x10400b490>
make_group = <function factory.<locals>.inner at 0x10400b640>
make_cluster_policy = <function factory.<locals>.inner at 0x10400b400>
make_cluster_policy_permissions = <function factory.<locals>.inner at 0x10400b7f0>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_running_real_validate_groups_permissions_job_fails(
        ws, sql_backend, new_installation, make_group, make_cluster_policy, make_cluster_policy_permissions
    ):
        ws_group_a = make_group()
    
        cluster_policy = make_cluster_policy()
        make_cluster_policy_permissions(
            object_id=cluster_policy.policy_id,
            permission_level=PermissionLevel.CAN_USE,
            group_name=ws_group_a.display_name,
        )
    
        generic_permissions = GenericPermissionsSupport(
            ws,
            [
                Listing(ws.cluster_policies.list, "policy_id", "cluster-policies"),
            ],
        )
    
>       install = new_installation(lambda wc: replace(wc, include_group_names=[ws_group_a.display_name]))

test_installation.py:257: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135793.213341, attempt = 32
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:29[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s44is[0m
[90m16:51:29[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_s44is: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s44is[0m
[90m16:51:29[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s44is', metastore_id=None, name='ucx_s44is', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_s44is
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_s44is: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_s44is
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s44is', metastore_id=None, name='ucx_s44is', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:30[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-Dwt5: https://DATABRICKS_HOST#setting/accounts/groups/994624443352375[0m
[90m16:51:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-Dwt5', entitlements=[], external_id=None, groups=[], id='994624443352375', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:30[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/00160CE140A9FE9B[0m
[90m16:51:30[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='00160CE140A9FE9B')[0m
[90m16:51:31[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster_policy permissions fixture: 00160CE140A9FE9B [group_name admins CAN_USE] -> [group_name sdk-Dwt5 CAN_USE][0m
[90m16:51:32[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.RRxi/config.yml) doesn't exist.[0m
[90m16:51:32[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:32[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:33[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-Dwt5: https://DATABRICKS_HOST#setting/accounts/groups/994624443352375
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-Dwt5', entitlements=[], external_id=None, groups=[], id='994624443352375', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/00160CE140A9FE9B
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='00160CE140A9FE9B')
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster_policy permissions fixture: 00160CE140A9FE9B [group_name admins CAN_USE] -> [group_name sdk-Dwt5 CAN_USE]
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.RRxi/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:33[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster_policy permissions fixtures[0m
[90m16:56:33[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster_policy permissions fixture: 00160CE140A9FE9B [group_name admins CAN_USE] -> [group_name sdk-Dwt5 CAN_USE][0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-Dwt5', entitlements=[], external_id=None, groups=[], id='994624443352375', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster policy fixtures[0m
[90m16:56:34[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='00160CE140A9FE9B')[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s44is', metastore_id=None, name='ucx_s44is', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:35[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s44is CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster_policy permissions fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster_policy permissions fixture: 00160CE140A9FE9B [group_name admins CAN_USE] -> [group_name sdk-Dwt5 CAN_USE]
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-Dwt5', entitlements=[], external_id=None, groups=[], id='994624443352375', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='00160CE140A9FE9B')
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_s44is', metastore_id=None, name='ucx_s44is', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_s44is CASCADE
_____________________ test_running_real_migrate_groups_job _____________________
[gw6] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

self = <urllib3.connectionpool.HTTPSConnectionPool object at 0x103aefeb0>
conn = <urllib3.connection.HTTPSConnection object at 0x1037bc8e0>
method = 'GET', url = '/api/2.0/clusters/list-node-types', body = None
headers = {'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service', 'Accept-Encoding':...ecjgJ2XIh4hPdNqoL1iW_Wguk7TUnzy0ItygvdpD3EyqEExMX5YBdiaQ91fxTpP8lczJ5bYaHlbz5H5L9XGHklRxE9PTBbrmv8-euIin0xxsHXJR_YtEw'}
retries = Retry(total=0, connect=None, read=False, redirect=None, status=None)
timeout = Timeout(connect=60, read=60, total=None), chunked = False
response_conn = <urllib3.connection.HTTPSConnection object at 0x1037bc8e0>
preload_content = False, decode_content = False, enforce_content_length = True

    def _make_request(
        self,
        conn: BaseHTTPConnection,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        retries: Retry | None = None,
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        chunked: bool = False,
        response_conn: BaseHTTPConnection | None = None,
        preload_content: bool = True,
        decode_content: bool = True,
        enforce_content_length: bool = True,
    ) -> BaseHTTPResponse:
        """
        Perform a request on a given urllib connection object taken from our
        pool.
    
        :param conn:
            a connection from one of our connection pools
    
        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)
    
        :param url:
            The URL to perform the request on.
    
        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.
    
        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.
    
        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.
    
            Pass ``None`` to retry until you receive a response. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.
    
            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.
    
        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
    
        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.
    
        :param chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.
    
        :param response_conn:
            Set this to ``None`` if you will handle releasing the connection or
            set the connection to have the response release it.
    
        :param preload_content:
          If True, the response's body will be preloaded during construction.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param enforce_content_length:
            Enforce content length checking. Body returned by server must match
            value of Content-Length header, if present. Otherwise, raise error.
        """
        self.num_requests += 1
    
        timeout_obj = self._get_timeout(timeout)
        timeout_obj.start_connect()
        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)
    
        try:
            # Trigger any extra validation we need to do.
            try:
                self._validate_conn(conn)
            except (SocketTimeout, BaseSSLError) as e:
                self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
                raise
    
        # _validate_conn() starts the connection to an HTTPS proxy
        # so we need to wrap errors with 'ProxyError' here too.
        except (
            OSError,
            NewConnectionError,
            TimeoutError,
            BaseSSLError,
            CertificateError,
            SSLError,
        ) as e:
            new_e: Exception = e
            if isinstance(e, (BaseSSLError, CertificateError)):
                new_e = SSLError(e)
            # If the connection didn't successfully connect to it's proxy
            # then there
            if isinstance(
                new_e, (OSError, NewConnectionError, TimeoutError, SSLError)
            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):
                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)
            raise new_e
    
        # conn.request() calls http.client.*.request, not the method in
        # urllib3.request. It also calls makefile (recv) on the socket.
        try:
            conn.request(
                method,
                url,
                body=body,
                headers=headers,
                chunked=chunked,
                preload_content=preload_content,
                decode_content=decode_content,
                enforce_content_length=enforce_content_length,
            )
    
        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is
        # legitimately able to close the connection after sending a valid response.
        # With this behaviour, the received response is still readable.
        except BrokenPipeError:
            pass
        except OSError as e:
            # MacOS/Linux
            # EPROTOTYPE and ECONNRESET are needed on macOS
            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/
            # Condition changed later to emit ECONNRESET instead of only EPROTOTYPE.
            if e.errno != errno.EPROTOTYPE and e.errno != errno.ECONNRESET:
                raise
    
        # Reset the timeout for the recv() on the socket
        read_timeout = timeout_obj.read_timeout
    
        if not conn.is_closed:
            # In Python 3 socket.py will catch EAGAIN and return None when you
            # try and read into the file pointer created by http.client, which
            # instead raises a BadStatusLine exception. Instead of catching
            # the exception and assuming all BadStatusLine exceptions are read
            # timeouts, check for a zero timeout before making the request.
            if read_timeout == 0:
                raise ReadTimeoutError(
                    self, url, f"Read timed out. (read timeout={read_timeout})"
                )
            conn.timeout = read_timeout
    
        # Receive the response from the server
        try:
>           response = conn.getresponse()

../../.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.venv/lib/python3.10/site-packages/urllib3/connection.py:466: in getresponse
    httplib_response = super().getresponse()
/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1374: in getresponse
    response.begin()
/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318: in begin
    version, status, reason = self._read_status()
/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279: in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705: in readinto
    return self._sock.recv_into(b)
/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274: in recv_into
    return self.read(nbytes, buffer)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0>
len = 8192, buffer = <memory at 0x103765300>

    def read(self, len=1024, buffer=None):
        """Read up to LEN bytes and return them.
        Return zero-length string on EOF."""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError("Read on closed or unwrapped SSL socket.")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               TimeoutError: The read operation timed out

/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130: TimeoutError

The above exception was the direct cause of the following exception:

self = <requests.adapters.HTTPAdapter object at 0x1035dfc10>
request = <PreparedRequest [GET]>, stream = False
timeout = Timeout(connect=60, read=60, total=None), verify = True, cert = None
proxies = OrderedDict()

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.
    
        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        """
    
        try:
            conn = self.get_connection(request.url, proxies)
        except LocationValueError as e:
            raise InvalidURL(e, request=request)
    
        self.cert_verify(conn, request.url, verify, cert)
        url = self.request_url(request, proxies)
        self.add_headers(
            request,
            stream=stream,
            timeout=timeout,
            verify=verify,
            cert=cert,
            proxies=proxies,
        )
    
        chunked = not (request.body is None or "Content-Length" in request.headers)
    
        if isinstance(timeout, tuple):
            try:
                connect, read = timeout
                timeout = TimeoutSauce(connect=connect, read=read)
            except ValueError:
                raise ValueError(
                    f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
                    f"or a single float to set both timeouts to the same value."
                )
        elif isinstance(timeout, TimeoutSauce):
            pass
        else:
            timeout = TimeoutSauce(connect=timeout, read=timeout)
    
        try:
>           resp = conn.urlopen(
                method=request.method,
                url=url,
                body=request.body,
                headers=request.headers,
                redirect=False,
                assert_same_host=False,
                preload_content=False,
                decode_content=False,
                retries=self.max_retries,
                timeout=timeout,
                chunked=chunked,
            )

../../.venv/lib/python3.10/site-packages/requests/adapters.py:486: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:847: in urlopen
    retries = retries.increment(
../../.venv/lib/python3.10/site-packages/urllib3/util/retry.py:470: in increment
    raise reraise(type(error), error, _stacktrace)
../../.venv/lib/python3.10/site-packages/urllib3/util/util.py:39: in reraise
    raise value
../../.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:793: in urlopen
    response = self._make_request(
../../.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:539: in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.connectionpool.HTTPSConnectionPool object at 0x103aefeb0>
err = TimeoutError('The read operation timed out')
url = '/api/2.0/clusters/list-node-types', timeout_value = 60

    def _raise_timeout(
        self,
        err: BaseSSLError | OSError | SocketTimeout,
        url: str,
        timeout_value: _TYPE_TIMEOUT | None,
    ) -> None:
        """Is the error actually a timeout? Will raise a ReadTimeout or pass"""
    
        if isinstance(err, SocketTimeout):
>           raise ReadTimeoutError(
                self, url, f"Read timed out. (read timeout={timeout_value})"
            ) from err
E           urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='DATABRICKS_HOST', port=443): Read timed out. (read timeout=60)

../../.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:370: ReadTimeoutError

During handling of the above exception, another exception occurred:

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135783.199285, attempt = 27
last_err = ReadTimeout(ReadTimeoutError("HTTPSConnectionPool(host='DATABRICKS_HOST', port=443): Read timed out. (read timeout=60)"))
retry_reason = 'timeout', sleep = 10, retry_after_secs = None

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:223: in _perform
    response = self._session.request(method,
../../.venv/lib/python3.10/site-packages/requests/sessions.py:589: in request
    resp = self.send(prep, **send_kwargs)
../../.venv/lib/python3.10/site-packages/requests/sessions.py:703: in send
    r = adapter.send(request, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <requests.adapters.HTTPAdapter object at 0x1035dfc10>
request = <PreparedRequest [GET]>, stream = False
timeout = Timeout(connect=60, read=60, total=None), verify = True, cert = None
proxies = OrderedDict()

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.
    
        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        """
    
        try:
            conn = self.get_connection(request.url, proxies)
        except LocationValueError as e:
            raise InvalidURL(e, request=request)
    
        self.cert_verify(conn, request.url, verify, cert)
        url = self.request_url(request, proxies)
        self.add_headers(
            request,
            stream=stream,
            timeout=timeout,
            verify=verify,
            cert=cert,
            proxies=proxies,
        )
    
        chunked = not (request.body is None or "Content-Length" in request.headers)
    
        if isinstance(timeout, tuple):
            try:
                connect, read = timeout
                timeout = TimeoutSauce(connect=connect, read=read)
            except ValueError:
                raise ValueError(
                    f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
                    f"or a single float to set both timeouts to the same value."
                )
        elif isinstance(timeout, TimeoutSauce):
            pass
        else:
            timeout = TimeoutSauce(connect=timeout, read=timeout)
    
        try:
            resp = conn.urlopen(
                method=request.method,
                url=url,
                body=request.body,
                headers=request.headers,
                redirect=False,
                assert_same_host=False,
                preload_content=False,
                decode_content=False,
                retries=self.max_retries,
                timeout=timeout,
                chunked=chunked,
            )
    
        except (ProtocolError, OSError) as err:
            raise ConnectionError(err, request=request)
    
        except MaxRetryError as e:
            if isinstance(e.reason, ConnectTimeoutError):
                # TODO: Remove this in 3.0.0: see #2811
                if not isinstance(e.reason, NewConnectionError):
                    raise ConnectTimeout(e, request=request)
    
            if isinstance(e.reason, ResponseError):
                raise RetryError(e, request=request)
    
            if isinstance(e.reason, _ProxyError):
                raise ProxyError(e, request=request)
    
            if isinstance(e.reason, _SSLError):
                # This branch is for urllib3 v1.22 and later.
                raise SSLError(e, request=request)
    
            raise ConnectionError(e, request=request)
    
        except ClosedPoolError as e:
            raise ConnectionError(e, request=request)
    
        except _ProxyError as e:
            raise ProxyError(e)
    
        except (_SSLError, _HTTPError) as e:
            if isinstance(e, _SSLError):
                # This branch is for urllib3 versions earlier than v1.22
                raise SSLError(e, request=request)
            elif isinstance(e, ReadTimeoutError):
>               raise ReadTimeout(e, request=request)
E               requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='DATABRICKS_HOST', port=443): Read timed out. (read timeout=60)

../../.venv/lib/python3.10/site-packages/requests/adapters.py:532: ReadTimeout

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x103aec580>
new_installation = <function new_installation.<locals>.factory at 0x1037f0940>
make_ucx_group = <function make_ucx_group.<locals>.inner at 0x1037f1120>
make_cluster_policy = <function factory.<locals>.inner at 0x1037f08b0>
make_cluster_policy_permissions = <function factory.<locals>.inner at 0x1037f13f0>

    @retried(on=[NotFound, Unknown, InvalidParameterValue], timeout=timedelta(minutes=5))
    def test_running_real_migrate_groups_job(
        ws, sql_backend, new_installation, make_ucx_group, make_cluster_policy, make_cluster_policy_permissions
    ):
        ws_group_a, acc_group_a = make_ucx_group()
    
        # perhaps we also want to do table grants here (to test acl cluster)
        cluster_policy = make_cluster_policy()
        make_cluster_policy_permissions(
            object_id=cluster_policy.policy_id,
            permission_level=PermissionLevel.CAN_USE,
            group_name=ws_group_a.display_name,
        )
    
        generic_permissions = GenericPermissionsSupport(
            ws,
            [
                Listing(ws.cluster_policies.list, "policy_id", "cluster-policies"),
            ],
        )
    
>       install = new_installation(lambda wc: replace(wc, include_group_names=[ws_group_a.display_name]))

test_installation.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135783.199285, attempt = 27
last_err = ReadTimeout(ReadTimeoutError("HTTPSConnectionPool(host='DATABRICKS_HOST', port=443): Read timed out. (read timeout=60)"))
retry_reason = 'timeout', sleep = 10, retry_after_secs = None

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:16[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_shdts[0m
[90m16:51:17[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_shdts: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_shdts[0m
[90m16:51:17[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_shdts', metastore_id=None, name='ucx_shdts', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_shdts
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_shdts: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_shdts
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_shdts', metastore_id=None, name='ucx_shdts', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:18[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-vwqp@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-vwqp@example.com')], entitlements=[], external_id=None, groups=[], id='8044833014635848', name=Name(family_name=None, given_name='sdk-vwqp@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-vwqp@example.com')[0m
[90m16:51:19[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group ucx_DHBB: https://DATABRICKS_HOST#setting/accounts/groups/2788700580869[0m
[90m16:51:19[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_DHBB', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='2788700580869', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:19[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Account group ucx_DHBB: https://accounts.CLOUD_ENVdatabricks.net/users/groups/167975437991619/members[0m
[90m16:51:19[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added account group fixture: Group(display_name='ucx_DHBB', entitlements=[], external_id=None, groups=[], id='167975437991619', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:51:20[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/001206995EE4F066[0m
[90m16:51:20[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='001206995EE4F066')[0m
[90m16:51:21[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added cluster_policy permissions fixture: 001206995EE4F066 [group_name admins CAN_USE] -> [group_name ucx_DHBB CAN_USE][0m
[90m16:51:22[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Cw9W/config.yml) doesn't exist.[0m
[90m16:51:22[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:22[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:23[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace user fixture: User(active=True, display_name='sdk-vwqp@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-vwqp@example.com')], entitlements=[], external_id=None, groups=[], id='8044833014635848', name=Name(family_name=None, given_name='sdk-vwqp@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-vwqp@example.com')
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group ucx_DHBB: https://DATABRICKS_HOST#setting/accounts/groups/2788700580869
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='ucx_DHBB', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='2788700580869', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Account group ucx_DHBB: https://accounts.CLOUD_ENVdatabricks.net/users/groups/167975437991619/members
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added account group fixture: Group(display_name='ucx_DHBB', entitlements=[], external_id=None, groups=[], id='167975437991619', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Cluster policy: https://DATABRICKS_HOST#setting/clusters/cluster-policies/view/001206995EE4F066
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster policy fixture: CreatePolicyResponse(policy_id='001206995EE4F066')
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added cluster_policy permissions fixture: 001206995EE4F066 [group_name admins CAN_USE] -> [group_name ucx_DHBB CAN_USE]
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Cw9W/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:37[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster_policy permissions fixtures[0m
[90m16:56:37[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster_policy permissions fixture: 001206995EE4F066 [group_name admins CAN_USE] -> [group_name ucx_DHBB CAN_USE][0m
[90m16:56:37[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace user fixtures[0m
[90m16:56:37[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-vwqp@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-vwqp@example.com')], entitlements=[], external_id=None, groups=[], id='8044833014635848', name=Name(family_name=None, given_name='sdk-vwqp@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-vwqp@example.com')[0m
[90m16:56:38[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 account group fixtures[0m
[90m16:56:38[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing account group fixture: Group(display_name='ucx_DHBB', entitlements=[], external_id=None, groups=[], id='167975437991619', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:56:39[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_DHBB', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='2788700580869', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:56:40[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 cluster policy fixtures[0m
[90m16:56:40[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='001206995EE4F066')[0m
[90m16:56:40[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:56:40[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_shdts', metastore_id=None, name='ucx_shdts', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:40[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_shdts CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster_policy permissions fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster_policy permissions fixture: 001206995EE4F066 [group_name admins CAN_USE] -> [group_name ucx_DHBB CAN_USE]
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace user fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace user fixture: User(active=True, display_name='sdk-vwqp@example.com', emails=[ComplexValue(display=None, primary=True, ref=None, type='work', value='sdk-vwqp@example.com')], entitlements=[], external_id=None, groups=[], id='8044833014635848', name=Name(family_name=None, given_name='sdk-vwqp@example.com'), roles=[], schemas=[<UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_USER: 'urn:ietf:params:scim:schemas:core:2.0:User'>, <UserSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_EXTENSION_WORKSPACE_2_0_USER: 'urn:ietf:params:scim:schemas:extension:workspace:2.0:User'>], user_name='sdk-vwqp@example.com')
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 account group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing account group fixture: Group(display_name='ucx_DHBB', entitlements=[], external_id=None, groups=[], id='167975437991619', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=None, roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='ucx_DHBB', entitlements=[ComplexValue(display=None, primary=None, ref=None, type=None, value='allow-cluster-create')], external_id=None, groups=[], id='2788700580869', members=[ComplexValue(display='sdk-vwqp@example.com', primary=None, ref='Users/8044833014635848', type=None, value='8044833014635848')], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing cluster policy fixture: CreatePolicyResponse(policy_id='001206995EE4F066')
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_shdts', metastore_id=None, name='ucx_shdts', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_shdts CASCADE
_________________________ test_repair_run_workflow_job _________________________
[gw5] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135811.211493, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x10715d690>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
mocker = <pytest_mock.plugin.MockerFixture object at 0x106e66ef0>
new_installation = <function new_installation.<locals>.factory at 0x10754dbd0>
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x10715e3b0>

    @retried(on=[NotFound, InvalidParameterValue], timeout=timedelta(minutes=10))
    def test_repair_run_workflow_job(ws, mocker, new_installation, sql_backend):
>       install = new_installation()

test_installation.py:292: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135811.211493, attempt = 26
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:48[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sd1uj[0m
[90m16:51:49[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sd1uj: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sd1uj[0m
[90m16:51:49[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sd1uj', metastore_id=None, name='ucx_sd1uj', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sd1uj
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sd1uj: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sd1uj
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sd1uj', metastore_id=None, name='ucx_sd1uj', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:50[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.7zu5/config.yml) doesn't exist.[0m
[90m16:51:50[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:50[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:51[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.7zu5/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:53[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:56:53[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:56:53[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sd1uj', metastore_id=None, name='ucx_sd1uj', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:53[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sd1uj CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sd1uj', metastore_id=None, name='ucx_sd1uj', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sd1uj CASCADE
_____________________________ test_uninstallation ______________________________
[gw3] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135806.689854, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x1054f7fa0>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
sql_backend = <databricks.labs.ucx.framework.crawlers.StatementExecutionBackend object at 0x105610df0>
new_installation = <function new_installation.<locals>.factory at 0x10581ae60>

    @retried(on=[NotFound], timeout=timedelta(minutes=5))
    def test_uninstallation(ws, sql_backend, new_installation):
>       install = new_installation()

test_installation.py:314: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135806.689854, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:51:43[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sxpw9[0m
[90m16:51:44[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_sxpw9: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sxpw9[0m
[90m16:51:44[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sxpw9', metastore_id=None, name='ucx_sxpw9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:51 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_sxpw9
16:51 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_sxpw9: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_sxpw9
16:51 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sxpw9', metastore_id=None, name='ucx_sxpw9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:51:45[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Sj2a/config.yml) doesn't exist.[0m
[90m16:51:45[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:51:45[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:51:46[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:51 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Sj2a/config.yml) doesn't exist.
16:51 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:51 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:51 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:56:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:56:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:56:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sxpw9', metastore_id=None, name='ucx_sxpw9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:56:54[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sxpw9 CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:56 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_sxpw9', metastore_id=None, name='ucx_sxpw9', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:56 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_sxpw9 CASCADE
___________________________ test_verify_permissions ____________________________
[gw7] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135825.03614, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x103c39660>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_group = <function factory.<locals>.inner at 0x10443a320>
make_job = <function factory.<locals>.inner at 0x104439d80>
make_job_permissions = <function factory.<locals>.inner at 0x104439fc0>

    @retried(on=[BadRequest], timeout=timedelta(minutes=3))
    def test_verify_permissions(ws, make_group, make_job, make_job_permissions):
        group_a = make_group()
>       job = make_job()

workspace_access/test_generic.py:377: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:755: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135825.03614, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
----------------------------- Captured stderr call -----------------------------
[90m16:52:05[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-38eg: https://DATABRICKS_HOST#setting/accounts/groups/894853401128987[0m
[90m16:52:05[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-38eg', entitlements=[], external_id=None, groups=[], id='894853401128987', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
------------------------------ Captured log call -------------------------------
16:52 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-38eg: https://DATABRICKS_HOST#setting/accounts/groups/894853401128987
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-38eg', entitlements=[], external_id=None, groups=[], id='894853401128987', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
--------------------------- Captured stderr teardown ---------------------------
[90m16:57:10[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job permissions fixtures[0m
[90m16:57:10[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 job fixtures[0m
[90m16:57:10[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 notebook fixtures[0m
[90m16:57:10[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 workspace group fixtures[0m
[90m16:57:10[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-38eg', entitlements=[], external_id=None, groups=[], id='894853401128987', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
---------------------------- Captured log teardown -----------------------------
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job permissions fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 job fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 notebook fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 workspace group fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-38eg', entitlements=[], external_id=None, groups=[], id='894853401128987', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
___________________________ test_job_cluster_policy ____________________________
[gw8] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135833.289272, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x107dbbd90>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
new_installation = <function new_installation.<locals>.factory at 0x110235e10>

    @retried(on=[NotFound, Unknown, InvalidParameterValue], timeout=timedelta(minutes=3))
    def test_job_cluster_policy(ws, new_installation):
>       install = new_installation(lambda wc: replace(wc, override_clusters=None))

test_installation.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
test_installation.py:70: in factory
    workspace_config = installer.configure()
../../src/databricks/labs/ucx/install.py:246: in configure
    policy_id = self._create_cluster_policy(inventory_database, spark_conf_dict, instance_profile)
../../src/databricks/labs/ucx/install.py:287: in _create_cluster_policy
    definition=self._cluster_policy_definition(conf=spark_conf, instance_profile=instance_profile),
../../src/databricks/labs/ucx/install.py:295: in _cluster_policy_definition
    "node_type_id": self._policy_config(self._ws.clusters.select_node_type(local_disk=True)),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135833.289272, attempt = 33
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
---------------------------- Captured stderr setup -----------------------------
[90m16:52:10[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_skgfn[0m
[90m16:52:11[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Schema hive_metastore.ucx_skgfn: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_skgfn[0m
[90m16:52:11[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_skgfn', metastore_id=None, name='ucx_skgfn', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
------------------------------ Captured log setup ------------------------------
16:52 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] CREATE SCHEMA hive_metastore.ucx_skgfn
16:52 INFO [databricks.labs.ucx.mixins.fixtures] Schema hive_metastore.ucx_skgfn: https://DATABRICKS_HOST/explore/data/hive_metastore/ucx_skgfn
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] added schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_skgfn', metastore_id=None, name='ucx_skgfn', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
----------------------------- Captured stderr call -----------------------------
[90m16:52:12[0m [1m[36mDEBUG[0m [90m[d.l.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Bii8/config.yml) doesn't exist.[0m
[90m16:52:12[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Please answer a couple of questions to configure Unity Catalog migration[0m
[90m16:52:12[0m [1m[32m INFO[0m [1m[d.l.u.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.[0m
[90m16:52:13[0m [1m[32m INFO[0m [1m[d.l.ucx.install] Creating UCX cluster policy.[0m
------------------------------ Captured log call -------------------------------
16:52 DEBUG [databricks.labs.ucx.install] Cannot find previous installation: Path (/Users/william.conti@databricks.com/.Bii8/config.yml) doesn't exist.
16:52 INFO [databricks.labs.ucx.install] Please answer a couple of questions to configure Unity Catalog migration
16:52 INFO [databricks.labs.ucx.installer.hms_lineage] HMS Lineage feature creates one system table named system.hms_to_uc_migration.table_access and helps in your migration process from HMS to UC by allowing you to programmatically query HMS lineage data.
16:52 INFO [databricks.labs.ucx.install] Creating UCX cluster policy.
--------------------------- Captured stderr teardown ---------------------------
[90m16:57:22[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 cluster policy fixtures[0m
[90m16:57:22[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 schema fixtures[0m
[90m16:57:22[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_skgfn', metastore_id=None, name='ucx_skgfn', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)[0m
[90m16:57:22[0m [1m[36mDEBUG[0m [90m[d.l.u.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_skgfn CASCADE[0m
---------------------------- Captured log teardown -----------------------------
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 cluster policy fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 schema fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] removing schema fixture: SchemaInfo(catalog_name='hive_metastore', catalog_type=None, comment=None, created_at=None, created_by=None, effective_predictive_optimization_flag=None, enable_predictive_optimization=None, full_name='hive_metastore.ucx_skgfn', metastore_id=None, name='ucx_skgfn', owner=None, properties=None, storage_location=None, storage_root=None, updated_at=None, updated_by=None)
16:57 DEBUG [databricks.labs.ucx.framework.crawlers] [api][execute] DROP SCHEMA IF EXISTS hive_metastore.ucx_skgfn CASCADE
________________________________ test_pipelines ________________________________
[gw1] darwin -- Python 3.10.9 /Users/william.conti/Projects/ucx/.venv/bin/python

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135864.4287019, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
>               return func(*args, **kwargs)

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <databricks.sdk.core.ApiClient object at 0x109837370>, method = 'GET'
path = '/api/2.0/clusters/list-node-types', query = None
headers = {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}
body = None, raw = False, files = None, data = None

    def _perform(self,
                 method: str,
                 path: str,
                 query: dict = None,
                 headers: dict = None,
                 body: dict = None,
                 raw: bool = False,
                 files=None,
                 data=None):
        response = self._session.request(method,
                                         f"{self._cfg.host}{path}",
                                         params=self._fix_query_string(query),
                                         json=body,
                                         headers=headers,
                                         files=files,
                                         data=data,
                                         stream=raw,
                                         timeout=self._http_timeout_seconds)
        try:
            self._record_request_log(response, raw=raw or data is not None or files is not None)
            if not response.ok: # internally calls response.raise_for_status()
                # TODO: experiment with traceback pruning for better readability
                # See https://stackoverflow.com/a/58821552/277035
                payload = response.json()
>               raise self._make_nicer_error(response=response, **payload) from None
E               databricks.sdk.errors.platform.TemporarilyUnavailable: Request timeout after 8 seconds

../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:238: TemporarilyUnavailable

The above exception was the direct cause of the following exception:

ws = WorkspaceClient(host='https://DATABRICKS_HOST', auth_type='metadata-service', ...)
make_group = <function factory.<locals>.inner at 0x10a0768c0>
make_pipeline = <function factory.<locals>.inner at 0x10a076d40>
make_pipeline_permissions = <function factory.<locals>.inner at 0x10a076e60>

    @retried(on=[NotFound], timeout=timedelta(minutes=3))
    def test_pipelines(ws, make_group, make_pipeline, make_pipeline_permissions):
        group_a = make_group()
        group_b = make_group()
>       pipeline = make_pipeline()

workspace_access/test_generic.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../src/databricks/labs/ucx/mixins/fixtures.py:55: in inner
    x = create(**kwargs)
../../src/databricks/labs/ucx/mixins/fixtures.py:796: in create
    node_type_id=ws.clusters.select_node_type(local_disk=True),
../../.venv/lib/python3.10/site-packages/databricks/sdk/mixins/compute.py:175: in select_node_type
    res = self.list_node_types()
../../.venv/lib/python3.10/site-packages/databricks/sdk/service/compute.py:6582: in list_node_types
    res = self._api.do('GET', '/api/2.0/clusters/list-node-types', headers=headers)
../../.venv/lib/python3.10/site-packages/databricks/sdk/core.py:130: in do
    response = retryable(self._perform)(method,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('GET', '/api/2.0/clusters/list-node-types')
kwargs = {'body': None, 'data': None, 'files': None, 'headers': {'Accept': 'application/json', 'User-Agent': 'ucx/0.13.2 databricks-sdk-py/0.20.0 python/3.10.9 os/darwin auth/metadata-service'}, ...}
deadline = 1709135864.4287019, attempt = 27
last_err = TemporarilyUnavailable('Request timeout after 8 seconds')
retry_reason = 'throttled by platform', sleep = 1, retry_after_secs = 1

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        deadline = clock.time() + timeout.total_seconds()
        attempt = 1
        last_err = None
        while clock.time() < deadline:
            try:
                return func(*args, **kwargs)
            except Exception as err:
                last_err = err
                retry_reason = None
                # sleep 10s max per attempt, unless it's HTTP 429 or 503
                sleep = min(10, attempt)
                retry_after_secs = getattr(err, 'retry_after_secs', None)
                if retry_after_secs is not None:
                    # cannot depend on DatabricksError directly because of circular dependency
                    sleep = retry_after_secs
                    retry_reason = 'throttled by platform'
                elif is_retryable is not None:
                    retry_reason = is_retryable(err)
                elif on is not None:
                    for err_type in on:
                        if not isinstance(err, err_type):
                            continue
                        retry_reason = f'{type(err).__name__} is allowed to retry'
    
                if retry_reason is None:
                    # raise if exception is not retryable
                    raise err
    
                logger.debug(f'Retrying: {retry_reason} (sleeping ~{sleep}s)')
                clock.sleep(sleep + random())
                attempt += 1
>       raise TimeoutError(f'Timed out after {timeout}') from last_err
E       TimeoutError: Timed out after 0:05:00

../../.venv/lib/python3.10/site-packages/databricks/sdk/retries.py:59: TimeoutError
----------------------------- Captured stderr call -----------------------------
[90m16:52:43[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-7GOe: https://DATABRICKS_HOST#setting/accounts/groups/160801322989092[0m
[90m16:52:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-7GOe', entitlements=[], external_id=None, groups=[], id='160801322989092', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:52:43[0m [1m[32m INFO[0m [1m[d.l.u.mixins.fixtures] Workspace group sdk-LaBO: https://DATABRICKS_HOST#setting/accounts/groups/25792050447461[0m
[90m16:52:43[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-LaBO', entitlements=[], external_id=None, groups=[], id='25792050447461', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:52:44[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-Az9Z.py[0m
------------------------------ Captured log call -------------------------------
16:52 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-7GOe: https://DATABRICKS_HOST#setting/accounts/groups/160801322989092
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-7GOe', entitlements=[], external_id=None, groups=[], id='160801322989092', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:52 INFO [databricks.labs.ucx.mixins.fixtures] Workspace group sdk-LaBO: https://DATABRICKS_HOST#setting/accounts/groups/25792050447461
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] added workspace group fixture: Group(display_name='sdk-LaBO', entitlements=[], external_id=None, groups=[], id='25792050447461', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:52 DEBUG [databricks.labs.ucx.mixins.fixtures] added notebook fixture: /Users/william.conti@databricks.com/sdk-Az9Z.py
--------------------------- Captured stderr teardown ---------------------------
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 pipeline permissions fixtures[0m
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 0 delta live table fixtures[0m
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 1 notebook fixtures[0m
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-Az9Z.py[0m
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] clearing 2 workspace group fixtures[0m
[90m16:57:54[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-7GOe', entitlements=[], external_id=None, groups=[], id='160801322989092', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
[90m16:58:01[0m [1m[36mDEBUG[0m [90m[d.l.u.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-LaBO', entitlements=[], external_id=None, groups=[], id='25792050447461', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])[0m
---------------------------- Captured log teardown -----------------------------
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 pipeline permissions fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 0 delta live table fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 1 notebook fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] removing notebook fixture: /Users/william.conti@databricks.com/sdk-Az9Z.py
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] clearing 2 workspace group fixtures
16:57 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-7GOe', entitlements=[], external_id=None, groups=[], id='160801322989092', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
16:58 DEBUG [databricks.labs.ucx.mixins.fixtures] removing workspace group fixture: Group(display_name='sdk-LaBO', entitlements=[], external_id=None, groups=[], id='25792050447461', members=[], meta=ResourceMeta(resource_type='WorkspaceGroup'), roles=[], schemas=[<GroupSchema.URN_IETF_PARAMS_SCIM_SCHEMAS_CORE_2_0_GROUP: 'urn:ietf:params:scim:schemas:core:2.0:Group'>])
=========================== short test summary info ============================
FAILED assessment/test_pipelines.py::test_pipeline_crawler - TimeoutError: Ti...
FAILED assessment/test_CLOUD_ENV.py::test_spn_crawler - TimeoutError: Timed out a...
FAILED assessment/test_CLOUD_ENV.py::test_spn_crawler_with_available_secrets - Ti...
FAILED framework/test_fixtures.py::test_job - TimeoutError: Timed out after 0...
FAILED assessment/test_clusters.py::test_cluster_crawler_no_isolation - Timeo...
FAILED assessment/test_CLOUD_ENV.py::test_spn_crawler_deleted_cluster_policy - Ti...
FAILED framework/test_fixtures.py::test_instance_pool - TimeoutError: Timed o...
FAILED assessment/test_CLOUD_ENV.py::test_spn_crawler_no_config - TimeoutError: T...
FAILED assessment/test_pipelines.py::test_pipeline_with_secret_conf_crawler
FAILED framework/test_fixtures.py::test_pipeline - TimeoutError: Timed out af...
FAILED assessment/test_clusters.py::test_cluster_crawler - TimeoutError: Time...
FAILED assessment/test_CLOUD_ENV.py::test_spn_crawler_with_pipeline_unavailable_secret
FAILED assessment/test_jobs.py::test_job_crawler - TimeoutError: Timed out af...
FAILED workspace_access/test_generic.py::test_instance_pools - TimeoutError: ...
FAILED test_installation.py::test_job_failure_propagates_correct_error_message_and_logs
FAILED workspace_access/test_generic.py::test_jobs - TimeoutError: Timed out ...
FAILED test_installation.py::test_running_real_assessment_job - TimeoutError:...
FAILED test_installation.py::test_running_real_validate_groups_permissions_job
FAILED test_installation.py::test_running_real_remove_backup_groups_job - Tim...
FAILED test_installation.py::test_running_real_validate_groups_permissions_job_fails
FAILED test_installation.py::test_running_real_migrate_groups_job - TimeoutEr...
FAILED test_installation.py::test_repair_run_workflow_job - TimeoutError: Tim...
FAILED test_installation.py::test_uninstallation - TimeoutError: Timed out af...
FAILED workspace_access/test_generic.py::test_verify_permissions - TimeoutErr...
FAILED test_installation.py::test_job_cluster_policy - TimeoutError: Timed ou...
FAILED workspace_access/test_generic.py::test_pipelines - TimeoutError: Timed...
============ 26 failed, 79 passed, 9 skipped in 1206.84s (0:20:06) =============
